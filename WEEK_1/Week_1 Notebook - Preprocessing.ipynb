{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP - Week 1\n",
    "## Cleaning and preprocessing textual data\n",
    "\n",
    "#### In this weeks session we will cover some fundamentals to get started with natural language processing. This includes cleaning and preprocessing textual data in order to make further analysis easier and more efficient.\n",
    "\n",
    "###### We will cover:\n",
    "- Removal of unwanted characters\n",
    "- Normalisation\n",
    "- Tokenisation into n-grams\n",
    "- Stopword removal\n",
    "- Lemmatising/Stemming\n",
    "\n",
    "#### In order to give you as many different examples and show cases as possible I will not apply all this to one particular dataset but rather to individual textual examples, since text cleaning is task specific and there is no \"one-size-fits-all\" solution. In the practial you will apply suitable preprocessing techniques to a whole dataset of your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = pd.read_csv(\"dummy_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sms</td>\n",
       "      <td>GENT! We are trying to contact you. Last weeke...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sms</td>\n",
       "      <td>Wa, ur openin sentence very formal... Anyway, ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sms</td>\n",
       "      <td>As I entered my cabin my PA said, '' Happy B'd...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sms</td>\n",
       "      <td>You are a winner U have been specially selecte...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sms</td>\n",
       "      <td>Goodo! Yes we must speak friday - egg-potato r...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sms</td>\n",
       "      <td>Hmm...my uncle just informed me that he's payi...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sms</td>\n",
       "      <td>PRIVATE! Your 2004 Account Statement for 07742...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sms</td>\n",
       "      <td>URGENT! Your Mobile No. was awarded σú2000 Bon...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sms</td>\n",
       "      <td>here is my new address -apples&amp;pairs&amp;all that ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sms</td>\n",
       "      <td>Todays Voda numbers ending 7548 are selected t...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>review</td>\n",
       "      <td>I WAS SO DISAPPOINTED!! ABSOLUTELY AWEFUL SERV...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>review</td>\n",
       "      <td>the food was good</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>review</td>\n",
       "      <td>THE FOOD WAS SO GOOD!!!!!! &lt;3&lt;3</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>review</td>\n",
       "      <td>not very nice staff, food was ok</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>review</td>\n",
       "      <td>I didn’t like it at all – food was cold and st...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>review</td>\n",
       "      <td>BEST PLACE EVER</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review</td>\n",
       "      <td>my friends liked it but I not so much</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review</td>\n",
       "      <td>I liked the food but the music was too loud an...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>review</td>\n",
       "      <td>:):):) like!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review</td>\n",
       "      <td>waiting time too long, table too small, overal...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>news_article</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>news_article</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>news_article</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>news_article</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>news_article</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>news_article</td>\n",
       "      <td>howard  truanted to play snooker  conservative...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>news_article</td>\n",
       "      <td>wales silent on grand slam talk rhys williams ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>news_article</td>\n",
       "      <td>french honour for director parker british film...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>news_article</td>\n",
       "      <td>car giant hit by mercedes slump a slump in pro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>news_article</td>\n",
       "      <td>fockers fuel festive film chart comedy meet th...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            type                                               text  \\\n",
       "0            sms  GENT! We are trying to contact you. Last weeke...   \n",
       "1            sms  Wa, ur openin sentence very formal... Anyway, ...   \n",
       "2            sms  As I entered my cabin my PA said, '' Happy B'd...   \n",
       "3            sms  You are a winner U have been specially selecte...   \n",
       "4            sms  Goodo! Yes we must speak friday - egg-potato r...   \n",
       "5            sms  Hmm...my uncle just informed me that he's payi...   \n",
       "6            sms  PRIVATE! Your 2004 Account Statement for 07742...   \n",
       "7            sms  URGENT! Your Mobile No. was awarded σú2000 Bon...   \n",
       "8            sms  here is my new address -apples&pairs&all that ...   \n",
       "9            sms  Todays Voda numbers ending 7548 are selected t...   \n",
       "10        review  I WAS SO DISAPPOINTED!! ABSOLUTELY AWEFUL SERV...   \n",
       "11        review                                  the food was good   \n",
       "12        review                    THE FOOD WAS SO GOOD!!!!!! <3<3   \n",
       "13        review                   not very nice staff, food was ok   \n",
       "14        review  I didn’t like it at all – food was cold and st...   \n",
       "15        review                                    BEST PLACE EVER   \n",
       "16        review              my friends liked it but I not so much   \n",
       "17        review  I liked the food but the music was too loud an...   \n",
       "18        review                                       :):):) like!   \n",
       "19        review  waiting time too long, table too small, overal...   \n",
       "20  news_article  worldcom ex-boss launches defence lawyers defe...   \n",
       "21  news_article  german business confidence slides german busin...   \n",
       "22  news_article  bbc poll indicates economic gloom citizens in ...   \n",
       "23  news_article  lifestyle  governs mobile choice  faster  bett...   \n",
       "24  news_article  enron bosses in $168m payout eighteen former e...   \n",
       "25  news_article  howard  truanted to play snooker  conservative...   \n",
       "26  news_article  wales silent on grand slam talk rhys williams ...   \n",
       "27  news_article  french honour for director parker british film...   \n",
       "28  news_article  car giant hit by mercedes slump a slump in pro...   \n",
       "29  news_article  fockers fuel festive film chart comedy meet th...   \n",
       "\n",
       "              tag  \n",
       "0            spam  \n",
       "1             ham  \n",
       "2             ham  \n",
       "3            spam  \n",
       "4             ham  \n",
       "5             ham  \n",
       "6            spam  \n",
       "7            spam  \n",
       "8             ham  \n",
       "9            spam  \n",
       "10       negative  \n",
       "11       positive  \n",
       "12       positive  \n",
       "13        neutral  \n",
       "14       negative  \n",
       "15       positive  \n",
       "16        neutral  \n",
       "17        neutral  \n",
       "18       positive  \n",
       "19       negative  \n",
       "20       business  \n",
       "21       business  \n",
       "22       business  \n",
       "23           tech  \n",
       "24       business  \n",
       "25       politics  \n",
       "26          sport  \n",
       "27  entertainment  \n",
       "28       business  \n",
       "29  entertainment  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the three types of text data in 3 separate dataframes\n",
    "sms_df = dummy_data[dummy_data['type'] == \"sms\"]\n",
    "review_df = dummy_data[dummy_data['type'] == \"review\"]\n",
    "news_df = dummy_data[dummy_data['type'] == \"news_article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of each for testing out stuff before applying it to the whole column\n",
    "\n",
    "sms_sample = \"\"\"***** CONGRATlations **** You won 2 tIckETs to Hamilton in \n",
    "NYC http://www.hamiltonbroadway.com/J?NaIOl/event   wORtH over $500.00...CALL \n",
    "555-477-8914 or send message to: hamilton@freetix.com to get ticket !! !\"\"\"\n",
    "review_sample = \"\"\" THIS FOOD AND STAFF WAS AMAZING!!!!! ABSOLUTELY LOVE THAT PLACE <3<3<3\"\"\"\n",
    "news_sample = \"\"\"worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (┬ú5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Removing unwanted chatacters - which characters to remove?\n",
    "The is a primary step in the process of text cleaning. If we scrap some text from HTML/XML sources, we’ll need to get rid of all the tags, HTML entities, punctuation, non-alphabets, and any other kind of characters which might not be a part of the language. The general methods of such cleaning involve regular expressions, which can be used to filter out most of the unwanted texts.\n",
    "\n",
    "However, sometimes, depending on the type of data, we want to retain certain types of punctuation. Consider for example human generated tweets which you want to classify as very angry, angry, neutral, happy, and very happy. Simple sentiment analysis might find it hard to differentiate between a happy, and a very happy sentiment, because the only difference between a happy and a very happy tweet might be punctuation.\n",
    "\n",
    "Example:\n",
    "\n",
    "*This is amazing*    <pre>  vs     </pre>  *THIS IS AMAZING!!!!!*\n",
    "\n",
    "---\n",
    "\n",
    "Or what about this one\n",
    "\n",
    "\n",
    "\n",
    "*I don't know :) <3* <pre>  vs     </pre>  *I don't know :(((*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expression keeping only letters \n",
    "\n",
    "def keep_letters_only(raw_text):\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    return letters_only_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      CONGRATlations      You won   tIckETs to Hamilton in  NYC http   www hamiltonbroadway com J NaIOl event   wORtH over           CALL               or send message to  hamilton freetix com to get ticket     '"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_letters_only(sms_sample) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### you can see that this is not ideal as this leaves us with a lot of random stuff like \"www\" and \"com\". We will get back to that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' THIS FOOD AND STAFF WAS AMAZING      ABSOLUTELY LOVE THAT PLACE       '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_letters_only(review_sample) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We don't lose any meaning, but as mentioned previously, keeping the exclamation marks might be useful if we want to distinguish between positive and *VERY* positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worldcom ex boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness   cynthia cooper  worldcom s ex head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in       her warnings led to the collapse of the firm following the discovery of an    bn       bn  accounting fraud  mr ebbers has pleaded not guilty to charges of fraud and conspiracy   prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates  but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early      and       she said andersen had given a  green light  to the procedures and practices used by worldcom  mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems   ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself  the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books   however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a      audit committee meeting  mr ebbers could face a jail sentence of    years if convicted of all the charges he is facing  worldcom emerged from bankruptcy protection in       and is now known as mci  last week  mci agreed to a buyout by verizon communications in a deal valued at      bn '"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_letters_only(news_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for news articles that works perfectly fine as we do not lose any relevant information in this case since we want to classify by genre (sports, business, tech etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalisation\n",
    "Recall our sms sample:\n",
    "\n",
    "**** **** CONGRATlations **** You won 2 tIckETs to Hamilton in \n",
    "NYC http://www.hamiltonbroadway.com/J?NaIOl/event   wORtH over $500.00...CALL \n",
    "555-477-8914 or send message to: hamilton@freetix.com to get ticket !! !\n",
    "\n",
    "I'd definitely deem this as spam. But clearly there's a lot going on here: phone numbers, emails, website URLs, money amounts, and gratuitous whitespace and punctuation. Some terms are randomly capitalized, others are in all-caps. Since these terms might show up in any one of the training examples in countless forms, we need a way to ensure each training example is on equal footing via a preprocessing step called **normalisation**.\n",
    "\n",
    "Instead of removing the following terms, for each training example, let's replace them with a specific string.\n",
    "\n",
    "- Replace email addresses with `emailaddr`\n",
    "- Replace URLs with `httpaddr`\n",
    "- Replace money symbols with `moneysymb`\n",
    "- Replace phone numbers with `phonenumbr`\n",
    "- Replace numbers with `numbr`\n",
    "- get rid of all other punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation_sms(raw_text):\n",
    "    cleaned = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', raw_text)\n",
    "    cleaned = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr',\n",
    "                     cleaned)\n",
    "    cleaned = re.sub(r'£|\\$|\\€', 'moneysymb ', cleaned) #add whitespace\n",
    "    cleaned = re.sub(\n",
    "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
    "        'phonenumbr', cleaned)\n",
    "    cleaned = re.sub(r'\\d+(\\.\\d+)?', 'numbr', cleaned)\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", cleaned)\n",
    "    return letters_only_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      CONGRATlations      You won numbr tIckETs to Hamilton in  NYC httpaddr   wORtH over moneysymb numbr   CALL  phonenumbr or send message to  emailaddr to get ticket     '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalisation_sms(sms_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenisation\n",
    "Tokenisation is just the process of splitting a sentence into words.\n",
    "\n",
    "Text: *A bad day in London is still better than a bad day anywhere else*\n",
    "\n",
    "Tokens: `['a', 'bad', 'day', 'in', 'London', 'is', 'still', 'better', 'than', 'a', 'bad', 'day', 'anywhere', 'else']`\n",
    "\n",
    "This example not only divides the individual entities, but also gets rid of the capitalism involved (no pun intended). Capitalisation and De-capitalisation is again, dependent on the data and the task at hand. If we want to differentiate between any sentiments, then something written in uppercase might mean something different than something written in lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'bad',\n",
       " 'day',\n",
       " 'in',\n",
       " 'london',\n",
       " 'is',\n",
       " 'still',\n",
       " 'better',\n",
       " 'than',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " 'anywhere',\n",
       " 'else']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: If you have already removed all punctuations you can just use pythons inbuilt .split() function\n",
    "\"A bad day in London is still better than a bad day anywhere else\".lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'bad',\n",
       " 'day',\n",
       " 'in',\n",
       " 'london,',\n",
       " 'is',\n",
       " 'still',\n",
       " 'better',\n",
       " 'than',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " 'anywhere',\n",
       " 'else!']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#However, if you still have punctuation, look what happens\n",
    "\"A bad day in London, is still better than a bad day anywhere else!\".lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'London' and 'London,' are not the same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'bad',\n",
       " 'day',\n",
       " 'in',\n",
       " 'london',\n",
       " ',',\n",
       " 'is',\n",
       " 'still',\n",
       " 'better',\n",
       " 'than',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'day',\n",
       " 'anywhere',\n",
       " 'else',\n",
       " '!']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so it's better to use nltk's tokenize\n",
    "from nltk import word_tokenize, bigrams, trigrams\n",
    "\n",
    "nltk_tokens = word_tokenize(\"A bad day in London, is still better than a bad day anywhere else!\".lower())\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bad'), ('bad', 'day'), ('day', 'in'), ('in', 'london'), ('london', ','), (',', 'is'), ('is', 'still'), ('still', 'better'), ('better', 'than'), ('than', 'a'), ('a', 'bad'), ('bad', 'day'), ('day', 'anywhere'), ('anywhere', 'else'), ('else', '!')]\n",
      "[('a', 'bad', 'day'), ('bad', 'day', 'in'), ('day', 'in', 'london'), ('in', 'london', ','), ('london', ',', 'is'), (',', 'is', 'still'), ('is', 'still', 'better'), ('still', 'better', 'than'), ('better', 'than', 'a'), ('than', 'a', 'bad'), ('a', 'bad', 'day'), ('bad', 'day', 'anywhere'), ('day', 'anywhere', 'else'), ('anywhere', 'else', '!')]\n"
     ]
    }
   ],
   "source": [
    "print(list(bigrams(nltk_tokens)))\n",
    "print(list(trigrams(nltk_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stopword Removal\n",
    "Stopwords are the words which are used very frequently. Words like “of, are, the, it, is” are some examples of stopwords. In applications like document search engines and document classification, where keywords are more important than general terms, removing stopwords can be a good idea. However, if there’s some application about, for instance, songs lyrics search, or searching for specific quotes, stopwords can be important. \n",
    "\n",
    "“To be, or not not be” - Stopwords in such phrases actually play an important role, and hence, should not be dropped.\n",
    "\n",
    "Another example is negation. \"not\" is contained in many stopword lists, but deleting \"not\" out of a negative review can make a positive out of it.\n",
    "\n",
    "There are two common approaches of removing the stopwords, and both are fairly straightforward. One way is to count all the word occurrences, and providing a threshold value on the count, and getting rid of all the terms/words occurring more than the specified threshold value. The other way is to have a predetermined list of stopwords, which can be removed from the list of tokens/tokenised sentences. I personally, believe the second one is better, as determining thresholds can be quite difficult and you can use tf-idf (more on that next lesson) to weigh the importance of words.\n",
    "\n",
    "NLTK comes with many corpora, including a stopword list. This list contains around 200 terms. For my research, however, I use one that contains over 600 terms: http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a11-smart-stop-list/english.stop (I removed apostrophes as I remove punctuations before I remove stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = 'SmartStoplist.txt'\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "with open(stop_words_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        stop_words.extend(line.split())\n",
    "\n",
    "\n",
    "#lets remove stopwords in the news article\n",
    "news_sample_clean = []\n",
    "for word in news_sample.split():\n",
    "    if word not in stop_words:\n",
    "        news_sample_clean.append(word)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK alternative\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "filtered_sentence_nltk = [w for w in nltk_tokens if not w in stop_words_nltk]\n",
    "filetered_sentence_smart = [w for w in nltk_tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worldcom',\n",
       " 'ex-boss',\n",
       " 'launches',\n",
       " 'defence',\n",
       " 'lawyers',\n",
       " 'defending',\n",
       " 'worldcom',\n",
       " 'chief',\n",
       " 'bernie',\n",
       " 'ebbers',\n",
       " 'battery',\n",
       " 'fraud',\n",
       " 'charges',\n",
       " 'called',\n",
       " 'company',\n",
       " 'whistleblower',\n",
       " 'witness.',\n",
       " 'cynthia',\n",
       " 'cooper',\n",
       " 'worldcom',\n",
       " 'ex-head',\n",
       " 'internal',\n",
       " 'accounting',\n",
       " 'alerted',\n",
       " 'directors',\n",
       " 'irregular',\n",
       " 'accounting',\n",
       " 'practices',\n",
       " 'telecoms',\n",
       " 'giant',\n",
       " '2002.',\n",
       " 'warnings',\n",
       " 'led',\n",
       " 'collapse',\n",
       " 'firm',\n",
       " 'discovery',\n",
       " '$11bn',\n",
       " '(┬ú5.7bn)',\n",
       " 'accounting',\n",
       " 'fraud.',\n",
       " 'mr',\n",
       " 'ebbers',\n",
       " 'pleaded',\n",
       " 'guilty',\n",
       " 'charges',\n",
       " 'fraud',\n",
       " 'conspiracy.',\n",
       " 'prosecution',\n",
       " 'lawyers',\n",
       " 'argued',\n",
       " 'mr',\n",
       " 'ebbers',\n",
       " 'orchestrated',\n",
       " 'series',\n",
       " 'accounting',\n",
       " 'tricks',\n",
       " 'worldcom',\n",
       " 'ordering',\n",
       " 'employees',\n",
       " 'hide',\n",
       " 'expenses',\n",
       " 'inflate',\n",
       " 'revenues',\n",
       " 'meet',\n",
       " 'wall',\n",
       " 'street',\n",
       " 'earnings',\n",
       " 'estimates.',\n",
       " 'ms',\n",
       " 'cooper',\n",
       " 'runs',\n",
       " 'consulting',\n",
       " 'business',\n",
       " 'told',\n",
       " 'jury',\n",
       " 'york',\n",
       " 'wednesday',\n",
       " 'external',\n",
       " 'auditors',\n",
       " 'arthur',\n",
       " 'andersen',\n",
       " 'approved',\n",
       " 'worldcom',\n",
       " 'accounting',\n",
       " 'early',\n",
       " '2001',\n",
       " '2002.',\n",
       " 'andersen',\n",
       " 'green',\n",
       " 'light',\n",
       " 'procedures',\n",
       " 'practices',\n",
       " 'worldcom.',\n",
       " 'mr',\n",
       " 'ebber',\n",
       " 'lawyers',\n",
       " 'unaware',\n",
       " 'fraud',\n",
       " 'arguing',\n",
       " 'auditors',\n",
       " 'alert',\n",
       " 'problems.',\n",
       " 'ms',\n",
       " 'cooper',\n",
       " 'shareholder',\n",
       " 'meetings',\n",
       " 'mr',\n",
       " 'ebbers',\n",
       " 'passed',\n",
       " 'technical',\n",
       " 'questions',\n",
       " 'company',\n",
       " 'finance',\n",
       " 'chief',\n",
       " 'giving',\n",
       " 'answers',\n",
       " 'himself.',\n",
       " 'prosecution',\n",
       " 'star',\n",
       " 'witness',\n",
       " 'worldcom',\n",
       " 'financial',\n",
       " 'chief',\n",
       " 'scott',\n",
       " 'sullivan',\n",
       " 'mr',\n",
       " 'ebbers',\n",
       " 'ordered',\n",
       " 'accounting',\n",
       " 'adjustments',\n",
       " 'firm',\n",
       " 'telling',\n",
       " 'hit',\n",
       " 'books',\n",
       " '.',\n",
       " 'ms',\n",
       " 'cooper',\n",
       " 'mr',\n",
       " 'sullivan',\n",
       " 'mentioned',\n",
       " 'uncomfortable',\n",
       " 'worldcom',\n",
       " 'accounting',\n",
       " '2001',\n",
       " 'audit',\n",
       " 'committee',\n",
       " 'meeting.',\n",
       " 'mr',\n",
       " 'ebbers',\n",
       " 'face',\n",
       " 'jail',\n",
       " 'sentence',\n",
       " '85',\n",
       " 'years',\n",
       " 'convicted',\n",
       " 'charges',\n",
       " 'facing.',\n",
       " 'worldcom',\n",
       " 'emerged',\n",
       " 'bankruptcy',\n",
       " 'protection',\n",
       " '2004',\n",
       " 'mci.',\n",
       " 'week',\n",
       " 'mci',\n",
       " 'agreed',\n",
       " 'buyout',\n",
       " 'verizon',\n",
       " 'communications',\n",
       " 'deal',\n",
       " 'valued',\n",
       " '$6.75bn.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sample_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad',\n",
       " 'day',\n",
       " 'london',\n",
       " ',',\n",
       " 'still',\n",
       " 'better',\n",
       " 'bad',\n",
       " 'day',\n",
       " 'anywhere',\n",
       " 'else',\n",
       " '!']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence_nltk # using nltk stopwordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad', 'day', 'london', ',', 'bad', 'day', '!']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filetered_sentence_smart #using smartsportword list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lemmatising and Stemming\n",
    "Lemmatisation and stemming both refer to a process of reducing a word to its root. The difference is that stem might not be an actual word whereas, a lemma is an actual word. It’s a handy tool if you want to avoid treating different forms of the same word as different words, e.g. *love, loved, loving*\n",
    "\n",
    "**Lemmatising:** considered, considers, consider → “consider”\n",
    "\n",
    "**Stemming:** considered, considering, consider → “consid”\n",
    "\n",
    "I personally have never noticed a significat difference between lemmatising and stemming when training classifiers. However, I suggest you try out yourself. NLTK comes with many different in-built lemmatisers and stemmers, so just plug and play.\n",
    "\n",
    "A note of caution: WordNetLemmatizer requires a POS-tag. The default is set to \"noun\" and therefore doesn't work with other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consid\n",
      "considers\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word = \"considers\"\n",
    "word_2 = \"apple\"\n",
    "\n",
    "stemmed_word =  stemmer.stem(word)\n",
    "lemmatised_word = lemmatizer.lemmatize(word)\n",
    "\n",
    "stemmed_word_2 =  stemmer.stem(word_2)\n",
    "lemmatised_word_2 = lemmatizer.lemmatize(word_2)\n",
    "\n",
    "print(stemmed_word)\n",
    "print(lemmatised_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appl\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_word_2)\n",
    "print(lemmatised_word_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together - in one preprocess() function\n",
    "Now that we covered everything we need to know, we can combine everything into one function and apply it to the whole data. Let's keep it simple and write one for the news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_news(raw_text):\n",
    "    \n",
    "    #keeping only letters\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and tokenise\n",
    "    tokens = word_tokenize(letters_only_text.lower())\n",
    "    \n",
    "\n",
    "    cleaned_words = []\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # remove stopwords\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    # stemm or lemmatise words\n",
    "    stemmed_words = []\n",
    "    for word in cleaned_words:\n",
    "        word = stemmer.stem(word)\n",
    "        stemmed_words.append(word)\n",
    "    \n",
    "    # converting list back to string\n",
    "    return \" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (┬ú5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worldcom boss launch defenc lawyer defend worldcom chief berni ebber batteri fraud charg call compani whistleblow wit cynthia cooper worldcom head intern account alert director irregular account practic telecom giant warn led collaps firm discoveri bn bn account fraud mr ebber plead guilti charg fraud conspiraci prosecut lawyer argu mr ebber orchestr seri account trick worldcom order employe hide expens inflat revenu meet wall street earn estim ms cooper run consult busi told juri york wednesday extern auditor arthur andersen approv worldcom account earli andersen green light procedur practic worldcom mr ebber lawyer unawar fraud argu auditor alert problem ms cooper sharehold meet mr ebber pass technic question compani financ chief give answer prosecut star wit worldcom financi chief scott sullivan mr ebber order account adjust firm tell hit book ms cooper mr sullivan mention uncomfort worldcom account audit committe meet mr ebber face jail sentenc year convict charg face worldcom emerg bankruptci protect mci week mci agre buyout verizon commun deal valu bn'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_news(news_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "great - it works, let's apply it to the \"whole data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-83-6321705505e3>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  news_df['prep_text'] = news_df['text'].apply(preprocess_news)\n"
     ]
    }
   ],
   "source": [
    "news_df['prep_text'] = news_df['text'].apply(preprocess_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>prep_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>news_article</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss launch defenc lawyer defend worl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>news_article</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "      <td>german busi confid slide german busi confid fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>news_article</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "      <td>bbc poll econom gloom citizen major nation sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>news_article</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "      <td>lifestyl govern mobil choic faster funkier har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>news_article</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "      <td>enron boss payout eighteen enron director agre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>news_article</td>\n",
       "      <td>howard  truanted to play snooker  conservative...</td>\n",
       "      <td>politics</td>\n",
       "      <td>howard truant play snooker conserv leader mich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>news_article</td>\n",
       "      <td>wales silent on grand slam talk rhys williams ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>wale silent grand slam talk rhi william wale t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>news_article</td>\n",
       "      <td>french honour for director parker british film...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>french honour director parker british film dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>news_article</td>\n",
       "      <td>car giant hit by mercedes slump a slump in pro...</td>\n",
       "      <td>business</td>\n",
       "      <td>car giant hit merced slump slump profit luxuri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>news_article</td>\n",
       "      <td>fockers fuel festive film chart comedy meet th...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>focker fuel festiv film chart comedi meet fock...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            type                                               text  \\\n",
       "20  news_article  worldcom ex-boss launches defence lawyers defe...   \n",
       "21  news_article  german business confidence slides german busin...   \n",
       "22  news_article  bbc poll indicates economic gloom citizens in ...   \n",
       "23  news_article  lifestyle  governs mobile choice  faster  bett...   \n",
       "24  news_article  enron bosses in $168m payout eighteen former e...   \n",
       "25  news_article  howard  truanted to play snooker  conservative...   \n",
       "26  news_article  wales silent on grand slam talk rhys williams ...   \n",
       "27  news_article  french honour for director parker british film...   \n",
       "28  news_article  car giant hit by mercedes slump a slump in pro...   \n",
       "29  news_article  fockers fuel festive film chart comedy meet th...   \n",
       "\n",
       "              tag                                          prep_text  \n",
       "20       business  worldcom boss launch defenc lawyer defend worl...  \n",
       "21       business  german busi confid slide german busi confid fe...  \n",
       "22       business  bbc poll econom gloom citizen major nation sur...  \n",
       "23           tech  lifestyl govern mobil choic faster funkier har...  \n",
       "24       business  enron boss payout eighteen enron director agre...  \n",
       "25       politics  howard truant play snooker conserv leader mich...  \n",
       "26          sport  wale silent grand slam talk rhi william wale t...  \n",
       "27  entertainment  french honour director parker british film dir...  \n",
       "28       business  car giant hit merced slump slump profit luxuri...  \n",
       "29  entertainment  focker fuel festiv film chart comedi meet fock...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
